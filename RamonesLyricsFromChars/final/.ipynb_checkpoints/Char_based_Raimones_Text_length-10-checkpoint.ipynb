{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 149346\n",
      "total chars: 47\n"
     ]
    }
   ],
   "source": [
    "#path = get_file('all_lyrics_no-umlaut_edited2.txt', origin='http://research.komakino.ch/all_lyrics_no-umlaut_edited2.txt ')\n",
    "\n",
    "filename = 'all_lyrics_no-umlaut_edited2.txt'\n",
    "\n",
    "text = open(filename).read().lower()\n",
    "\n",
    "text = text.replace('\\x99', '')\n",
    "text = text.replace('\\x80', '')\n",
    "text = text.replace('\\xe2', '')\n",
    "\n",
    "\n",
    "text = text.replace('\"', '')\n",
    "text = text.replace('(', '')\n",
    "text = text.replace(')', '')\n",
    "text = text.replace('_', '')\n",
    "text = text.replace(';', ',')\n",
    "text = text.replace('&amp;', '&')\n",
    "\n",
    "\n",
    "\n",
    "#all_lyrics_no-umlaut_edited2.txt\n",
    "#path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "#text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "n_chars = len(text)\n",
    "n_vocab = len(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 10\n",
    "#step = 3\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 149336\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (i, sentence) in enumerate(sentences):\n",
    "    for (t, char) in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 80\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7680Epoch 00000: loss improved from inf to 0.76796, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7680.hdf5\n",
      "149336/149336 [==============================] - 87s - loss: 0.7680    \n",
      "\n",
      "----- diversity: 0.1\n",
      "----- Generating with seed: \"wanna shoc\"\n",
      "wanna shock treatment girl in my heart stood still and spiderman, spiderman, spiderman, spiderman, spiderman, spiderman, spiderman, spiderman, spiderman, spiderman, spiderman, spiderman, spiderman, spiderman, s\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"wanna shoc\"\n",
      "wanna shock treatment girl in my heart at the time is rlain\n",
      "\n",
      "i don't wanna grow up\n",
      "someone had happy forchhh.......\n",
      "well i tried and i tried\n",
      "i can't come back to set the time is rlain\n",
      "\n",
      "i want to steal from\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"wanna shoc\"\n",
      "wanna shock treatment girl in a call let's go let's go\n",
      "let's go\n",
      "let's go\n",
      "let's go\n",
      "let's go surfin' now\n",
      "everybody i love her so\n",
      "he hangin' out in life mand please\n",
      "and they what cappy cleip here with you\n",
      "\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"wanna shoc\"\n",
      "wanna shock treton, don't go\n",
      "don't leave me baby\n",
      "don't talk to me\n",
      "and eate edgy speundisoruuse you're fenite, i'm sick andittin' eatod\n",
      "\n",
      "can't stop this pies, i did now i wanna run away for the consy\n",
      "\n",
      "last n\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 81\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7683Epoch 00000: loss did not improve\n",
      "149336/149336 [==============================] - 91s - loss: 0.7684    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 82\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7656Epoch 00000: loss improved from 0.76796 to 0.76569, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7657.hdf5\n",
      "149336/149336 [==============================] - 91s - loss: 0.7657    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 83\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7637Epoch 00000: loss improved from 0.76569 to 0.76371, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7637.hdf5\n",
      "149336/149336 [==============================] - 94s - loss: 0.7637    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 84\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7633Epoch 00000: loss improved from 0.76371 to 0.76329, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7633.hdf5\n",
      "149336/149336 [==============================] - 93s - loss: 0.7633    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 85\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7644Epoch 00000: loss did not improve\n",
      "149336/149336 [==============================] - 95s - loss: 0.7644    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 86\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7631Epoch 00000: loss improved from 0.76329 to 0.76315, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7632.hdf5\n",
      "149336/149336 [==============================] - 100s - loss: 0.7632   \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 87\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7587Epoch 00000: loss improved from 0.76315 to 0.75867, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7587.hdf5\n",
      "149336/149336 [==============================] - 113s - loss: 0.7587   \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 88\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7614Epoch 00000: loss did not improve\n",
      "149336/149336 [==============================] - 113s - loss: 0.7614   \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 89\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7569Epoch 00000: loss improved from 0.75867 to 0.75696, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7570.hdf5\n",
      "149336/149336 [==============================] - 112s - loss: 0.7570   \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 90\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7575Epoch 00000: loss did not improve\n",
      "149336/149336 [==============================] - 109s - loss: 0.7576   \n",
      "\n",
      "----- diversity: 0.1\n",
      "----- Generating with seed: \"- ay, hay\n",
      "\"\n",
      "- ay, hay\n",
      "\n",
      "over my shoulder\n",
      "but i sta cry\n",
      "\n",
      "i want you around\n",
      "\n",
      "i want you around\n",
      "\n",
      "i am a tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu tu \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"- ay, hay\n",
      "\"\n",
      "- ay, hay\n",
      "\n",
      "over my shoulder\n",
      "but i said i loved like a bitter tastor\n",
      "and i can't control myself \n",
      "i can't remember you i remember you i remember you i remember you i remember you i remember you i remember you\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"- ay, hay\n",
      "\"\n",
      "- ay, hay\n",
      "\n",
      "never let her wath and strosting before and it's not me fallin' ding on somethioule\n",
      "\n",
      "i can't seem to make you mine\n",
      "come on safari to diugh merry choup like da\n",
      "\n",
      "i can't remember you i remember you\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"- ay, hay\n",
      "\"\n",
      "- ay, hay\n",
      "\n",
      "don't sain\n",
      "teely so the fun i lond man' bame comb\n",
      "spina anything now\n",
      "\n",
      "i can't usea\n",
      "and there oncy your mind is drig very out\n",
      "your fan\n",
      "and beated just a bcheing mental...\n",
      "sourmed than there's \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 91\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7543Epoch 00000: loss improved from 0.75696 to 0.75415, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7541.hdf5\n",
      "149336/149336 [==============================] - 107s - loss: 0.7541   \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 92\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7544Epoch 00000: loss did not improve\n",
      "149336/149336 [==============================] - 99s - loss: 0.7543    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 93\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7524Epoch 00000: loss improved from 0.75415 to 0.75263, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7526.hdf5\n",
      "149336/149336 [==============================] - 87s - loss: 0.7526    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 94\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7530Epoch 00000: loss did not improve\n",
      "149336/149336 [==============================] - 84s - loss: 0.7530    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 95\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7543Epoch 00000: loss did not improve\n",
      "149336/149336 [==============================] - 85s - loss: 0.7545    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 96\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7527Epoch 00000: loss did not improve\n",
      "149336/149336 [==============================] - 87s - loss: 0.7527    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 97\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7500Epoch 00000: loss improved from 0.75263 to 0.75016, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7502.hdf5\n",
      "149336/149336 [==============================] - 85s - loss: 0.7502    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 98\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7512Epoch 00000: loss did not improve\n",
      "149336/149336 [==============================] - 87s - loss: 0.7511    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 99\n",
      "Epoch 1/1\n",
      "149248/149336 [============================>.] - ETA: 0s - loss: 0.7493Epoch 00000: loss improved from 0.75016 to 0.74939, saving model to aws_128_00_length10_step1_batch128_weights-improvement-00-0.7494.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149336/149336 [==============================] - 89s - loss: 0.7494    \n"
     ]
    }
   ],
   "source": [
    "filepath=\"aws_128_00_length10_step1_batch128_weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min', period=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Word_weights-improvement-00-0.5946.hdf5\n",
    "\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(80, 100):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    history = model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=1, \n",
    "              callbacks=callbacks_list)\n",
    "\n",
    "    \n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    if (iteration%10==0):\n",
    "        for diversity in [0.1, 0.2, 0.5, 1.0]:\n",
    "            print()\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(200):\n",
    "                x = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.1\n",
      "\"---- Generating with seed: \" you know\n",
      " you know\n",
      "it's not me\n",
      "it's not for me to know it's true\n",
      "ignorens to lived something in my drink\n",
      "somebody put something in my drink\n",
      "somebody put something in my drink\n",
      "somebody put something in my drink\n",
      "somebody put something in my drink\n",
      "somebody put something in my drink\n",
      "somebody put something in my drink\n",
      "somebody put something in my drink\n",
      "somebody put something in my drink\n",
      "somebody put something\n",
      "\n",
      "----- diversity: 0.2\n",
      "\"---- Generating with seed: \" you know\n",
      " you know\n",
      "it's not me\n",
      "it's gonna be real find a tragay to the strength to endure\n",
      "and all the bars and they do now in the sperrived away from my drink...in my drink\n",
      "\n",
      "i don't wanna grow up\n",
      "i don't wanna grow up\n",
      "i don't wanna grow up\n",
      "i don't wanna grow up\n",
      "i know your biggest glame the dangers, its the dangers, its the dangers, its the dangers, its the dangers, its the dangers, its the dangers, its the \n",
      "\n",
      "----- diversity: 0.5\n",
      "\"---- Generating with seed: \" you know\n",
      " you know\n",
      "run around\n",
      "let me show you what you wanna dance the watw too\n",
      "\n",
      "and all the balchines on my oirend like a hapry oh yeah oh yea oh oh oh oh oh oh oh, oh oh oh\n",
      "it's alright, it's alright\n",
      "i am gonna be the end you see\n",
      "it's meas a plach\n",
      "stup the please me of that crimms and i tried and i tried and i tried and i tried and i tried\n",
      "come back into me\n",
      "but i know i know i know i know your back if you\n",
      "\n",
      "----- diversity: 1.0\n",
      "\"---- Generating with seed: \" you know\n",
      " you know\n",
      "she's the one\n",
      "summer run ignr, isn there\n",
      "just get blood be cons, yeah it'le going through now\n",
      "\n",
      "oh i can their wirl cand my howl is just can't stop 't me\n",
      "yelly stuc\n",
      "ther word \n",
      "too the law a compadin \n",
      "botter mind\n",
      "i got my side\n",
      "hopple, innace from mution\n",
      "but you're wind mysely\n",
      "i'm alive, i'm youg up\n",
      "\n",
      "she was everything to me, aw yeah\n",
      "\n",
      "i want the airwa-farimes in have up the piscerm\n",
      "it's\n"
     ]
    }
   ],
   "source": [
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "for diversity in [0.1, 0.2, 0.5, 1.0]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
