{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model_type = 'best'\n",
    "chooser = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/mfrey/Documents/GitHub/raimones/Raimones4Web'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 149346)\n",
      "('total chars:', 47)\n",
      "('nb sequences:', 149246)\n",
      "Vectorization...\n",
      "('nb sequences:', 49769)\n",
      "Vectorization...\n",
      "('nb sequences:', 149336)\n",
      "Vectorization...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = get_file('all_lyrics_no-umlaut_edited2.txt', origin='http://research.komakino.ch/all_lyrics_no-umlaut_edited2.txt ')\n",
    "\n",
    "filename = 'all_lyrics_no-umlaut_edited2.txt'\n",
    "\n",
    "text = open(filename).read().lower()\n",
    "\n",
    "text = text.replace('\\x99', '')\n",
    "text = text.replace('\\x80', '')\n",
    "text = text.replace('\\xe2', '')\n",
    "\n",
    "\n",
    "text = text.replace('\"', '')\n",
    "text = text.replace('(', '')\n",
    "text = text.replace(')', '')\n",
    "text = text.replace('_', '')\n",
    "text = text.replace(';', ',')\n",
    "text = text.replace('&amp;', '&')\n",
    "\n",
    "#all_lyrics_no-umlaut_edited2.txt\n",
    "#path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "#text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "\n",
    "#maxlen = 100\n",
    "#maxlen = 40\n",
    "#maxlen = 10\n",
    "\n",
    "#step = 3\n",
    "#step = 1\n",
    "\n",
    "\n",
    "################\n",
    "maxlen = 100\n",
    "step = 1\n",
    "\n",
    "sentences_100_1 = []\n",
    "next_chars_100_1 = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences_100_1.append(text[i: i + maxlen])\n",
    "    next_chars_100_1.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences_100_1))\n",
    "\n",
    "\n",
    "print('Vectorization...')\n",
    "X_100_1 = np.zeros((len(sentences_100_1), maxlen, len(chars)), dtype=np.bool)\n",
    "y_100_1 = np.zeros((len(sentences_100_1), len(chars)), dtype=np.bool)\n",
    "\n",
    "for (i, sentences_100_1) in enumerate(sentences_100_1):\n",
    "    for (t, char) in enumerate(sentence):\n",
    "        X_100_1[i, t, char_indices[char]] = 1\n",
    "        y_100_1[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "        \n",
    "################\n",
    "maxlen = 40\n",
    "step = 3\n",
    "\n",
    "sentences_40_3 = []\n",
    "next_chars_40_3 = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences_40_3.append(text[i: i + maxlen])\n",
    "    next_chars_40_3.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences_40_3))\n",
    "\n",
    "    \n",
    "print('Vectorization...')\n",
    "X_40_3 = np.zeros((len(sentences_40_3), maxlen, len(chars)), dtype=np.bool)\n",
    "y_40_3 = np.zeros((len(sentences_40_3), len(chars)), dtype=np.bool)\n",
    "\n",
    "\n",
    "for (i, sentence) in enumerate(sentences_40_3):\n",
    "    for (t, char) in enumerate(sentence):\n",
    "        X_40_3[i, t, char_indices[char]] = 1\n",
    "        y_40_3[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "##############\n",
    "maxlen = 10\n",
    "step = 1\n",
    "\n",
    "sentences_10_1 = []\n",
    "next_chars_10_1 = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences_10_1.append(text[i: i + maxlen])\n",
    "    next_chars_10_1.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences_10_1))\n",
    "\n",
    "        \n",
    "print('Vectorization...')\n",
    "X_10_1 = np.zeros((len(sentences_10_1), maxlen, len(chars)), dtype=np.bool)\n",
    "y_10_1 = np.zeros((len(sentences_10_1), len(chars)), dtype=np.bool)\n",
    "\n",
    "for (i, sentence) in enumerate(sentences_10_1):\n",
    "    for (t, char) in enumerate(sentence):\n",
    "        X_10_1[i, t, char_indices[char]] = 1\n",
    "        y_10_1[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "f = open('char-indeces.txt', 'w')\n",
    "\n",
    "toBeSaved =  [ char_indices, indices_char ]\n",
    "\n",
    "json.dump(toBeSaved, f)\n",
    "f.close()\n",
    "\n",
    "np.save('X1001.npy', X_100_1)\n",
    "np.save('X403.npy', X_40_3)\n",
    "np.save('X101.npy', X_10_1)\n",
    "\n",
    "type(char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149346\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(X_100_1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('char-indeces.txt', 'r')\n",
    "tbs = json.load(f)\n",
    "\n",
    "char_indices = tbs[0]\n",
    "indices_char = tbs[1]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('best', 'Model  ', 'maxlen:', 10, 'step:', 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "maxlen_list = [100, 100, 40, 40, 10]\n",
    "step_list =   [  1,   1,  3,  3,  1]\n",
    "\n",
    "modelname_best = ['lyrics_hdf5/aws_128-128_0.5_char_length100_step1_batch128_weights-improvement-90-1.4600.hdf5',\n",
    "                  'lyrics_hdf5/aws_256-256_0.4_char_length100_step1_batch128_weights-improvement-253-1.2255.hdf5',\n",
    "                  'lyrics_hdf5/aws_128-128_02_length40_step3_batch128_weights-improvement-00-0.7131.hdf5',\n",
    "                  'lyrics_hdf5/aws_128_00_length40_step3_batch128_weights-improvement-00-0.9602.hdf5', \n",
    "                  'lyrics_hdf5/aws_128_00_length10_step1_batch128_weights-improvement-00-0.7494.hdf5'\n",
    "                 ]\n",
    "\n",
    "modelname_worse = ['lyrics_hdf5/aws_128-128_0.5_char_length100_step1_batch128_weights-improvement-20-1.8126.hdf5',\n",
    "                   'lyrics_hdf5/aws_256-256_0.4_char_length100_step1_batch128_weights-improvement-10-1.8214.hdf5',\n",
    "                   'lyrics_hdf5/aws_128-128_02_length40_step3_batch128_weights-improvement-00-1.0104.hdf5',\n",
    "                   'lyrics_hdf5/aws_128_00_length40_step3_batch128_weights-improvement-00-1.1308.hdf5', \n",
    "                   'lyrics_hdf5/aws_128_00_length10_step1_batch128_weights-improvement-00-1.0064.hdf5'\n",
    "                  ]\n",
    "\n",
    "\n",
    "\n",
    "X_list = ['X1001.npy', 'X1001.npy','X403.npy', 'X403.npy', 'X101.npy' ]\n",
    "\n",
    "X = np.load(X_list[chooser])\n",
    "\n",
    "\n",
    "if model_type == 'best':\n",
    "    modelnamelist = modelname_best\n",
    "else: \n",
    "    modelnamelist = modelname_worse\n",
    "\n",
    "step = step_list[chooser]\n",
    "maxlen = maxlen_list[chooser]\n",
    "\n",
    "print(model_type, 'Model  ',\"maxlen:\",maxlen,\"step:\", step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(modelnamelist[chooser])\n",
    "print(\"... Model loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n"
     ]
    }
   ],
   "source": [
    "filepath=\"aws_128_00_length10_step1_batch128_weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min', period=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Word_weights-improvement-00-0.5946.hdf5\n",
    "\n",
    "\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(X)-1)\n",
    "pattern = X[start]\n",
    "print(\"Seed:\")\n",
    "#print(\"\\\"\", ''.join([indices_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "#pattern.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.T\n",
    "#len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "('----- diversity:', 0.1)\n",
      "----- Generating with seed: \"or\r\n",
      "now yo\"\n",
      "(1, 10, 47)\n",
      "done\n",
      "after prediction\n",
      "after sampling\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-cac7c040052a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after sampling'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 41"
     ]
    }
   ],
   "source": [
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "for diversity in [0.1, 0.2, 0.5, 1.0]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "    for i in range(300):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        print(x.shape)\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        print('done') \n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        print('after prediction')\n",
    "        next_index = sample(preds, diversity)\n",
    "        print('after sampling')\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print(generated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
